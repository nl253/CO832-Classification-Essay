% vim:wrap:textwidth=1000:
%% options: [no]titlepage, twocolumn, landscape, draft
\documentclass[a4paper, 12pt, titlepage]{article}
\usepackage{graphicx}
\usepackage{titlesec}

\titlespacing*{\section}
{0pt}{1.0ex plus 1ex minus .2ex}{1.0ex plus .2ex}
\titlespacing*{\subsection}
{0pt}{1.0ex plus 1ex minus .2ex}{1.0ex plus .2ex}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[margin=0.35in]{geometry}

\date{} % overwrite default format
\author{Norbert Logiewa\\nl253}
\title{Fairness-Aware Classification Algorithms}

\begin{document}

\maketitle

%% NOTES
%% --------------------------------------------------------------------------------
%%  2. If so, are the biases signification enough to justify the introduction of further complexity to already complex algorithms?
%% 
%%  3. Even if an algorithm consistently classifies e.g. single mothers as falling into the category of e.g. "working class", can we really blame it if this is what happens in the vast majority of cases in real life?
%% 
%%  4. In what sense can an algorithm be "unfair"? Classification algorithms will produce a model based on the data given to them. It might be that they consistently attach a label to a certain group of people, but if the data is an  accurate reflection of reality, then that label, in the vast majority of cases would be appropriate (e.g. single mothers being working class).


In recent years, researchers have began raising the issue of fairness and bias in classification. \cite{pedreshi2008}. There have been concerns about classification algorithms or some models created with those algorithm with some datasets being "unfair".

Classification is a supervised learning technique that relies on using existing data to construct a model to use for predictions. The use of data points and construction of a model based on them necessitates distinction between different instances (i.e. discrimination). The idea of classification is inherently discriminatory, however, in the context of this essay, to be unfair, discriminatory or bias will refer to classification based on \emph{protected attributes}. \footnote{The Equality Act lists: age, gender reassignment, being married or in a civil partnership, being pregnant or on maternity leave, disability, race including colour, nationality, ethnic or national origin, religion or belief, sex, sexual orientation as \emph{protected attributes}}. There are ethical and legal issues associated with this problem and some decisions that are made based on the output of classifiers may seriously affect people's lives (e.g. credit score). Therefore, the data mining community has began investigating ways in which we can remedy it which this essay will discuss.

% \section*{Origins of Bias}

% Indre pointed out that the bias is in the dataset and that an algorithm cannot be unfair in itself, they can, however be "discrimination-aware" if they employ measures to compat bias. This is, as he argued, because algorithms are not used for decision making, all they do is construct a model. Therefore, it is the model that may be biased. \cite[p.~7]{indre2015}

\section*{A Naive Solution}

A naive solution to the problem of sensitive features would be to remove them from the dataset. E.g. if we were in the business of estimating an individual's credit score and there was a dataset with columns: "FirstName", "LastName", "Age", "Race", "Salary", "HistoryOfIllness" ... based on which the credit score was to be computed, we would remove "Age" and "Race" to ensure that these characteristics don't influence the final decision.
Researchers have pointed out a flaw in this reasoning, namely that these removed sensitive features \emph{still} influence other features in an indirect fashion (e.g. age may be correlated with salary) \cite[p.~36]{kamishima2012a} \cite[p.~378]{kamishima2012b} \cite[p.~381]{kamishima2012b}. This effect has been termed \emph{red-lining effect} and is an instance of indirect discrimination. \cite[p.~380]{kamishima2012b}

% \section*{Biased Data}

% One of the potential root causes of bias in a classification task can be the dataset. Data can be collected in such a way that there is a lack of representation from certain individuals. For instance, a video streaming company might collect data about it's users. From that data, they might infer, that people of Asian descent have a preference for comedies. However the vast majority of the company's clients are from Europe. Since Asian people do not originate from Europe they are underrepresented. As a result, they might constitute a small fraction of the user base. If the company was to make predictions based on that data they might be biased by the particular behaviour of (relatively) small number of the Asian people living in Europe. The predictions that said company makes about Asians living in Asia may not be valid at all.

% The fundamental issue is that the traditional algorithms used in data mining do not focus on correcting errors or deficiencies of the dataset. In a sense they blindly trust the input data as a representation of reality. This might cause them to be misled by the errors in dataset.

% Questions that data mining experts should be asking themselves when making predictions are:

\section*{Measuring Fairness}

A major problem within the area of fairness-aware classification is measuring the extent of bias and fairness. Various suggestions have been put forward.
Kamishima et al. suggest that it may be formalized as lack of correlation between sensitive features and the outcome. \cite[p.~259]{kamishima2018}.

Indre in his survey discusses the various measures that have emerged to calculate the degree of bias. He categorises them into: statistical tests, absolute measures, conditional measures and structural measures. He argues that to determine whether the classifier is fair, we must have access to protected characteristics. \cite[p.~8]{indre2015}

A measure of fitness discussed by Kamishima was \emph{Calders and Verwer's Discrimination Score} (CV score). An approach might be taken to combat bias in classification is taking a measure of bias such as CV score \footnote{There are also other measures such as \emph{Disparate Impact (DI)}.} and tweaking the model in such a way that the score begins to approach 0. 

\section*{Accuracy vs Fairness}

The modifications to existing algorithms \emph{may} force us to sacrifice some accuracy.  An important part of devising a fairness-aware classifier is considering the trade-off between fairness and accuracy. The aim is to have a high level of fairness but still be able to make accurate predictions.  \cite[p.~259]{kamishima2018} \cite[p.~9]{menon2018}. 

\section*{Proposed Improvements}

A number of modification to existing algorithms have been proposed.

\subsection*{Calders and Verwer's two-naive- Bayes (CV2NB)}

\section*{Conclusion}

In summary efforts have been made to address the issue of fairness in classification. Different researchers have proposed different modifications to existing algorithms. There are also a number of different measures of bias. The research community does not seem to be agreement on which measure to use or even if this measure is a good reflection of the extent of "unfairness". This makes it difficult to verify that the proposed modified versions of ML algorithms really solve the problem.

\begin{thebibliography}{10}

  % PAPERS
  
  \bibitem{pedreshi2008}
  Dino Pedreshi, Salvatore Ruggieri, and Franco
Turini,
  Discrimination-aware data mining,
  In ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD),
  Pages 560 - 568, 
  2008.

  \bibitem{kamishima2012a}
  T. Kamishima, S. Akaho, H. Asoh and J. Sakuma,
  \textit{Considerations on Fairness-Aware Data Mining},
    IEEE 12th International Conference on Data Mining Workshops, Brussels,
    Pages 378 - 385,
    2012.

  \bibitem{kamishima2012b}
  Kamishima, Toshihiro and Akaho, Shotaro and Asoh, Hideki and Sakuma, Jun,
  \textit{Fairness-Aware Classifier with Prejudice Remover Regularizer}
  Machine Learning and Knowledge Discovery in Databases,
  Pages 35 - 50,
  Springer Berlin Heidelberg,
  2012.

  \bibitem{kamishima2018}
  Kamishima, Toshihiro and Akaho, Shotaro and Asoh, Hideki and Sakuma, Jun,
  \textit{Model-based and actual independence for fairness-aware classification},
  Data Mining and Knowledge Discovery,
  Volume 32,
  Pages 258 - 286,
  2018.

  \bibitem{kamishima2013}
  T. Kamishima, S. Akaho, H. Asoh and J. Sakuma, 
  \textit{The Independence of Fairness-Aware Classifiers},
  IEEE 13th International Conference on Data Mining Workshops, Dallas, TX,
  Pages 849 - 858,
  2013.

  \bibitem{menon2018}
  Aditya Krishna Menon and Robert C. Williamson,
  \textit{The Cost of Fairness in Binary Classification},
  Proceedings of Machine Learning Research,
  Conference on Fairness, Accountability, and Transparency,
  Pages 1 - 12,
  2018.

  \bibitem{indre2015}
  Zliobaite, Indre,
  \textit{A survey on measuring indirect discrimination in machine learning}, 
  2015. 

\end{thebibliography}

\end{document}
